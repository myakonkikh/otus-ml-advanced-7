{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d09a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c438525",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b50ee5",
   "metadata": {},
   "source": [
    "##  Описание среды Taxi-v3\n",
    "\n",
    "###  States\n",
    "\n",
    "Состояние среды кодируется целым числом от `0` до `499` и включает:\n",
    "\n",
    "- **Позицию такси**: 25 возможных клеток (сеткой 5×5)\n",
    "- **Положение пассажира**:\n",
    "  - `0`: у точки R (Red)\n",
    "  - `1`: у точки G (Green)\n",
    "  - `2`: у точки Y (Yellow)\n",
    "  - `3`: у точки B (Blue)\n",
    "  - `4`: внутри такси\n",
    "- **Целевая точка назначения**:\n",
    "  - `0`: R\n",
    "  - `1`: G\n",
    "  - `2`: Y\n",
    "  - `3`: B\n",
    "\n",
    "**Всего состояний:**  \n",
    "`25 (такси) × 5 (пассажир) × 4 (цель) = 500`\n",
    "\n",
    "**Формула кодирования состояния:**  \n",
    "`state = ((taxi_row * 5 + taxi_col) * 5 + passenger_pos) * 4 + destination`\n",
    "\n",
    "---\n",
    "\n",
    "### Actions\n",
    "\n",
    "| Код | Действие              |\n",
    "|-----|-----------------------|\n",
    "| `0` | Поехать вверх         |\n",
    "| `1` | Поехать вниз          |\n",
    "| `2` | Поехать влево         |\n",
    "| `3` | Поехать вправо        |\n",
    "| `4` | Забрать пассажира     |\n",
    "| `5` | Высадить пассажира    |\n",
    "\n",
    "---\n",
    "\n",
    "### Rewards\n",
    "\n",
    "| Событие                                         | Награда |\n",
    "|-------------------------------------------------|---------|\n",
    "| Любой обычный шаг                               | `-1`    |\n",
    "| Неверный `pickup` или `dropoff`                 | `-10`   |\n",
    "| Успешная высадка пассажира в точке назначения   | `+20`   |\n",
    "| Завершение эпизода                              | —       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457b2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(env, policy, episodes=10000):\n",
    "    total_reward = 0\n",
    "    successes = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = int(policy[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "        if reward == 20:  # Успешная доставка пассажира\n",
    "            successes += 1\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    return avg_reward, successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf827e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_one_episode(env, policy, delay=0.5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()  # показываем текущее состояние\n",
    "        time.sleep(delay)\n",
    "\n",
    "        action = int(policy[state])\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    env.render()  # показать финальное состояние\n",
    "    print(\"Завершено. Общая награда за эпизод:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4d0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    V = np.zeros(n_states)\n",
    "    policy = np.zeros(n_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0.\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            q_sa = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                for prob, next_s, reward, done in env.P[s][a]:\n",
    "                    q_sa[a] += prob * (reward + gamma * V[next_s])\n",
    "\n",
    "            V[s] = np.max(q_sa)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # извлечение политики\n",
    "    for s in range(n_states):\n",
    "        q_sa = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            for prob, next_s, reward, done in env.P[s][a]:\n",
    "                q_sa[a] += prob * (reward + gamma * V[next_s])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463c3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-3):\n",
    "    V = np.zeros(n_states)\n",
    "    policy = np.random.choice(n_actions, size=n_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0.\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            a = policy[s]\n",
    "            V[s] = np.sum([prob * (reward + gamma * V[next_s]) for prob, next_s, reward, done in env.P[s][a]])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "        \n",
    "        policy_stable = True\n",
    "        for s in range(n_states):\n",
    "            old_policy = policy[s]\n",
    "            q_sa = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                q_sa[a] = np.sum([prob * (reward + gamma * V[next_s]) for prob, next_s, reward, done in env.P[s][a]])\n",
    "            policy[s] = np.argmax(q_sa)\n",
    "\n",
    "            if old_policy != policy[s]:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcb7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env, episodes=10000, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns = {(s, a): [] for s in range(n_states) for a in range(n_actions)}\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc='Monte Carlo'):\n",
    "        episode = []\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Q[s])\n",
    "            next_s, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            episode.append((s, a, r))\n",
    "            s = next_s\n",
    "        \n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for s, a, r in reversed(episode):\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                visited.add((s, a))\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[s, a] = np.mean(returns[(s, a)])\n",
    "\n",
    "    return np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438de380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c92481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, episodes=10000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc='Q-learning'):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = epsilon_greedy(Q, s, epsilon)\n",
    "            next_s, r, terminated, truncated, info = env.step(a)\n",
    "            done = r == 20\n",
    "            Q[s, a] += alpha * (r + gamma * np.max(Q[next_s]) - Q[s, a])\n",
    "            s = next_s\n",
    "\n",
    "    return np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, episodes=10000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc='SARSA'):\n",
    "        s, _ = env.reset()\n",
    "        a = epsilon_greedy(Q, s, epsilon)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_s, r, terminated, truncated, info = env.step(a)\n",
    "            next_a = epsilon_greedy(Q, next_s, epsilon)\n",
    "            done = r == 20\n",
    "            Q[s, a] += alpha * (r + gamma * Q[next_s, next_a] - Q[s, a])\n",
    "            s, a = next_s, next_a\n",
    "\n",
    "    return np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c264577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monte Carlo: 100%|██████████| 10000/10000 [00:23<00:00, 425.29it/s]\n",
      "Q-learning: 100%|██████████| 10000/10000 [00:04<00:00, 2476.53it/s]\n",
      "SARSA: 100%|██████████| 10000/10000 [00:03<00:00, 3179.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration: Средняя награда = 7.92, Побед = 10000/10000\n",
      "Policy Iteration: Средняя награда = 7.95, Побед = 10000/10000\n",
      "Monte Carlo Policy Iteration: Средняя награда = -598.17, Побед = 498/10000\n",
      "Q-learning: Средняя награда = 7.90, Побед = 10000/10000\n",
      "SARSA: Средняя награда = 7.90, Побед = 10000/10000\n"
     ]
    }
   ],
   "source": [
    "policies = {\n",
    "    \"Value Iteration\": value_iteration(env),\n",
    "    \"Policy Iteration\": policy_iteration(env),\n",
    "    \"Monte Carlo Policy Iteration\": monte_carlo(env),\n",
    "    \"Q-learning\": q_learning(env),\n",
    "    \"SARSA\": sarsa(env)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, policy in policies.items():\n",
    "    avg_reward, wins = eval_policy(env, policy)\n",
    "    results[name] = (avg_reward, wins)\n",
    "    print(f\"{name}: Средняя награда = {avg_reward:.2f}, Побед = {wins}/10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74fec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SARSA: 100%|██████████| 10000/10000 [00:02<00:00, 3620.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда за эпизод: 7.93\n",
      "Успешных высадок пассажиров: 10000 / 10000\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = sarsa(env)\n",
    "avg_reward, success_count = eval_policy(env, optimal_policy)\n",
    "\n",
    "print(f\"Средняя награда за эпизод: {avg_reward:.2f}\")\n",
    "print(f\"Успешных высадок пассажиров: {success_count} / 10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "733d3dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завершено. Общая награда за эпизод: 7\n"
     ]
    }
   ],
   "source": [
    "render_env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "render_one_episode(render_env, optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157462e",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "В целом все алгоритмы кроме Monte Carlo справляются с задачей отлично"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
